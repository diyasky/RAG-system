#è°ƒapi
import faiss
import numpy as np
import os
import pickle
import logging
from openai import OpenAI  # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯
from loader import load_documents  # ä½¿ç”¨æ”¹è¿›åçš„ loader æ¨¡å—
from config import config

# ========================
# æ¨¡å—åˆå§‹åŒ–é…ç½®
# é…ç½®æ—¥å¿—è¾“å‡º
# ========================
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(api_key=config.API_KEY, base_url=config.BASE_URL)

def get_embedding_from_api(text):
    """é€šè¿‡ API è·å–åµŒå…¥å‘é‡ï¼Œç¡®ä¿æ–‡æœ¬é•¿åº¦åœ¨æœ‰æ•ˆèŒƒå›´å†…"""
    max_input_length = 2048  # OpenAI æœ€å¤§è¾“å…¥é•¿åº¦
    # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œè¿›è¡Œæ‹†åˆ†
    if len(text) > max_input_length:
        chunks = [text[i:i + max_input_length] for i in range(0, len(text), max_input_length)]
        embeddings = []
        for chunk in chunks:
            response = client.embeddings.create(
                model=config.EMBEDDING_MODEL,
                input=chunk
            )
            # è·å–åµŒå…¥å‘é‡
            embeddings.append(np.array(response.data[0].embedding))  # ä½¿ç”¨ response.data[0].embedding è·å–å‘é‡
        return np.mean(embeddings, axis=0)  # è¿”å›æ‰€æœ‰åˆ†å—åµŒå…¥å‘é‡çš„å¹³å‡å€¼
    else:
        response = client.embeddings.create(
            model=config.EMBEDDING_MODEL,
            input=text
        )
        # è·å–åµŒå…¥å‘é‡
        return np.array(response.data[0].embedding)  # ä½¿ç”¨ response.data[0].embedding è·å–å‘é‡

def rebuild_index():
    """é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶é‡å»º FAISS ç´¢å¼•"""
    print("ğŸ”„ å¼€å§‹é‡å»º FAISS ç´¢å¼•...")

    # ä½¿ç”¨é…ç½®ä¸­çš„çŸ¥è¯†åº“ç›®å½•
    knowledge_dir = config.REFERENCE_FOLDER  # è¿™é‡Œæ”¹ä¸º `REFERENCE_FOLDER`

    # é€šè¿‡ loader å¹¶å‘åŠ è½½æ‰€æœ‰æ–‡æ¡£
    docs = list(load_documents(knowledge_dir))
    if not docs:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œç´¢å¼•æœªæ›´æ–°ã€‚")
        return "âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ï¼Œç´¢å¼•æœªæ›´æ–°ã€‚"

    print(f"ğŸ“‚ åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£")  # æ‰“å°æ–‡æ¡£æ•°é‡

    # æå–æ¯ç¯‡æ–‡æ¡£çš„å†…å®¹å’Œæ–‡ä»¶å
    texts = [doc["content"] for doc in docs]
    filenames = [doc["filename"] for doc in docs]

    # å°†æ–‡æ¡£å†…å®¹è½¬åŒ–ä¸ºåµŒå…¥å‘é‡
    embeddings = np.array([get_embedding_from_api(text) for text in texts])  # è·å–åµŒå…¥å‘é‡

    # ä½¿ç”¨ FAISS å»ºç«‹ç´¢å¼•
    dim = embeddings.shape[1]  # å‘é‡ç»´åº¦
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    # ä¿å­˜ FAISS ç´¢å¼•å’Œæ–‡æ¡£æ–‡ä»¶ååˆ—è¡¨
    faiss.write_index(index, str(config.FAISS_CACHE / "docs.index"))

    with open(str(config.FAISS_CACHE / "filenames.pkl"), "wb") as f:
        pickle.dump(filenames, f)

    print("âœ… FAISS ç´¢å¼•å·²æˆåŠŸé‡å»ºï¼")
    return "âœ… FAISS ç´¢å¼•å·²æˆåŠŸé‡å»ºï¼"


# å¦‚æœ `main.py` ç›´æ¥è¿è¡Œï¼Œåˆ™è‡ªåŠ¨åˆ›å»ºç´¢å¼•
if __name__ == "__main__":
    rebuild_index()


