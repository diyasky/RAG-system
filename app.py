import gradio as gr
import os
import torch
import psutil  # ç”¨äºç³»ç»Ÿç›‘æ§
from rag import generate_answer
from config import config
from loader import load_documents
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import json
from main import rebuild_index  # âœ… ç›´æ¥å¯¼å…¥ `rebuild_index`
import shutil
import time

# 1ï¸å¤´åƒè·¯å¾„
USER_ICON_PATH = "icon/user.png"
BOT_ICON_PATH = "icon/bot.png"


# 2ï¸ åŠ è½½ LLM æ¨¡å‹
def load_llm_model():
    """é€šè¿‡ API åŠ¨æ€åŠ è½½ LLM"""
    # ç°åœ¨ä¸å†åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ API è·å– LLM å“åº”
    return None, None  # ä¸éœ€è¦è¿”å›æ¨¡å‹å’Œtokenizer


# é¢„åŠ è½½é»˜è®¤ LLMï¼ˆå®é™…ä¸Šæ­¤æ—¶ä¸å†éœ€è¦åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ï¼‰
tokenizer, model = load_llm_model()


# 3ï¸ å¤„ç†æ–‡ä»¶ä¸Šä¼ 
def upload_files(files, chatbot):
    """ä¸Šä¼ æ–‡ä»¶ï¼Œæ›´æ–° Chatbot å¹¶ç¡®ä¿è¿”å›ç¬¦åˆ Gradio Chatbot æ ¼å¼"""
    if not isinstance(files, list):
        files = [files]

    saved_files = []
    failed_files = []

    for file in files:
        try:
            original_filename = file.orig_name if hasattr(file, "orig_name") else os.path.basename(file.name)
            dest_path = os.path.join(config.REFERENCE_FOLDER, original_filename)

            # âœ… å°†ä¸´æ—¶è·¯å¾„æ–‡ä»¶ç§»åŠ¨åˆ° `knowledge_base` ç›®å½•
            shutil.move(file.name, dest_path)

            saved_files.append(original_filename)
        except Exception as e:
            failed_files.append(original_filename)
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {original_filename}, é”™è¯¯: {e}")

    # âœ… åªæœ‰è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ‰é‡å»ºç´¢å¼•
    if saved_files:
        print("ğŸ”„ è‡³å°‘ä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹é‡å»ºç´¢å¼•...")
        index_message = rebuild_index()
    else:
        index_message = "âš ï¸ æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œç´¢å¼•æœªæ›´æ–°ã€‚"

    # âœ… æ„å»ºè¿”å›æ¶ˆæ¯
    message = f"ğŸ“‚ ä¸Šä¼ æˆåŠŸ {len(saved_files)} ä¸ªæ–‡ä»¶: {', '.join(saved_files)}"
    if failed_files:
        message += f"\nâŒ ä¸Šä¼ å¤±è´¥ {len(failed_files)} ä¸ªæ–‡ä»¶: {', '.join(failed_files)}"

    message += f"\n{index_message}"
    print(message)

    # âœ… è®© Chatbot æ˜¾ç¤ºæ¶ˆæ¯ï¼ˆæ ¼å¼å¿…é¡»æ˜¯ `[(ç”¨æˆ·è¾“å…¥, æœºå™¨äººå›å¤)]`ï¼‰
    chatbot.append(("ğŸ“ æ–‡ä»¶ä¸Šä¼ ", message))
    return chatbot


# 4ï¸ å¤„ç†å¯¹è¯
# def chat_with_rag(question, chatbot, max_tokens, temperature, top_p, show_debug, topk_retrieval, dist_threshold):
#     chatbot.append((question, ""))
#     start_time = time.time()
#
#     stream = generate_answer(question)
#     bot_response = ""
#
#     for current_output in stream:
#         if "</think>" in current_output:
#             parts = current_output.split("</think>")
#             if len(parts) >= 2:
#                 bot_response = parts[1].strip()  # æ¨¡å‹çš„å›ç­”
#                 debug_info = parts[2].strip() if len(parts) > 2 else "æš‚æ— æ¨ç†è¿‡ç¨‹"
#             else:
#                 bot_response = parts[0].strip()
#                 debug_info = "æš‚æ— æ¨ç†è¿‡ç¨‹"
#         else:
#             bot_response = current_output.strip()
#
#         chatbot[-1] = (question, bot_response)
#         yield "", chatbot
#
#     # æ¨ç†æ—¶é—´è®¡ç®—
#     elapsed_time = time.time() - start_time
#     elapsed_str = f"ğŸ” ç‚¹å‡»æŸ¥çœ‹æ¨ç†è¿‡ç¨‹ï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ âŒ„"
#
#     # âœ… ä»…å¯¹æ¨ç†è¿‡ç¨‹çš„å­—ä½“è¿›è¡Œæ·¡åŒ–ï¼ˆå»é™¤å¯¹ bot_response çš„é¢å¤–é¢œè‰²ä¿®æ”¹ï¼‰
#     if show_debug and debug_info:
#         bot_response = (
#             f"<details>"
#             f"<summary style='color:#888;font-size:12px;'>{elapsed_str}</summary>"
#             f"<div style='color:#ccc;background:#f5f5f5;padding:10px;border-radius:5px;'>\n\n"
#             f"{debug_info}\n"
#             f"</div></details>\n\n"
#             f"{bot_response}"  # âœ… å»æ‰äº† bot_response å¤–éƒ¨çš„é¢å¤–é¢œè‰²è®¾ç½®
#         )
#         chatbot[-1] = (question, bot_response)
#         yield "", chatbot
async def chat_with_rag(question, chatbot, max_tokens, temperature, top_p, show_debug, topk_retrieval, dist_threshold):
    chatbot.append((question, "Processing..."))
    yield "", chatbot  # Yielding before processing to show that it's working

    start_time = time.time()

    # Simulate generating an answer (use the actual model logic here)
    stream = generate_answer(question)  # Get response from model

    bot_response = ""
    debug_info = ""

    # Collect all output chunks and concatenate them
    for current_output in stream:
        print(f"Generated output: {current_output}")  # Debugging line to check the model's output

        bot_response += current_output.strip()  # Concatenate each chunk to build the full response

    if not bot_response:  # If no valid response, set a fallback message
        bot_response = "I'm sorry, I couldn't generate a response."

    chatbot[-1] = (question, bot_response)
    yield "", chatbot  # Yielding the concatenated response

    # Inference time calculation
    elapsed_time = time.time() - start_time
    elapsed_str = f"â³ Processed in {elapsed_time:.2f} seconds."

    if show_debug and debug_info:
        bot_response = (
            f"<details>"
            f"<summary style='color:#888;font-size:12px;'>{elapsed_str}</summary>"
            f"<div style='color:#ccc;background:#f5f5f5;padding:10px;border-radius:5px;'>\n\n"
            f"{debug_info}\n"
            f"</div></details>\n\n"
            f"{bot_response}"
        )
        chatbot[-1] = (question, bot_response)
        yield "", chatbot  # Yield final response with reasoning (if any)




# 5ï¸ åˆ‡æ¢ LLM æ¨¡å‹
def switch_model(new_model):
    """åˆ‡æ¢ LLM æ¨¡å‹"""
    # é€šè¿‡ API è®¾ç½®æ–°çš„æ¨¡å‹ï¼ˆä¸å†ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼‰
    config.LLM_MODEL_NAME = new_model
    return f"âœ… å·²åˆ‡æ¢åˆ° {new_model} æ¨¡å‹"


# 6ï¸ ç³»ç»Ÿç›‘æ§
def system_diagnosis():
    """è¿”å› CPUã€RAMã€GPU èµ„æºä½¿ç”¨æƒ…å†µ"""
    cpu_usage = psutil.cpu_percent()
    ram_usage = psutil.virtual_memory().percent
    gpu_usage = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0
    return {"CPU ä½¿ç”¨ç‡": f"{cpu_usage}%", "å†…å­˜ä½¿ç”¨ç‡": f"{ram_usage}%", "GPU å ç”¨": f"{gpu_usage:.2f}GB"}


# 7ï¸ å¯¼å‡º & å¯¼å…¥å¯¹è¯å†å²
def export_chat_history(chatbot):
    """å¯¼å‡ºå¯¹è¯å†å²ä¸º JSON æ–‡ä»¶"""
    history = [{"ç”¨æˆ·": msg[0], "AI": msg[1]} for msg in chatbot]
    file_path = "chat_history.json"
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    return file_path


def import_chat_history(file):
    """å¯¼å…¥å¯¹è¯å†å²"""
    if file is None:
        return []

    with open(file.name, "r", encoding="utf-8") as f:
        history = json.load(f)

    return [(msg["ç”¨æˆ·"], msg["AI"]) for msg in history]


# ========================
# ğŸš€ Gradio ç•Œé¢æ„å»º
# ========================
def create_gradio_interface():
    """æ„å»ºäº¤äº’ç•Œé¢"""
    theme = gr.themes.Default(
        primary_hue="orange",
        secondary_hue="blue"
    )

    with gr.Blocks(theme=theme, title="DeepSeek RAG System 2.0") as interface:
        gr.Markdown("# ğŸ” åŸºäºLLMs API çš„ RAG çŸ¥è¯†é—®ç­”ç³»ç»Ÿ (DeepSeekä¹ä¸ç‰ˆ)")

        # å¯¹è¯åŒº
        chatbot = gr.Chatbot(
            value=[(None, "æ‚¨å¥½ï¼æˆ‘æ˜¯èƒ–æ©˜ï¼Œæ‚¨çš„æ™ºèƒ½åŠ©æ‰‹ ğŸš€")],
            height=680,
            avatar_images=(USER_ICON_PATH, BOT_ICON_PATH)
        )
        msg_input = gr.Textbox(placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=3)

        with gr.Row():
            submit_btn = gr.Button("ğŸ’¬ å‘é€", variant="primary")
            os.environ["GRADIO_MAX_FILE_SIZE"] = "100mb"
            upload_btn = gr.UploadButton(
                "ğŸ“ ä¸Šä¼ æ–‡æ¡£",
                file_types=[".pdf", ".docx", ".txt", ".md", ".pptx", ".xlsx"],
                file_count="multiple"
            )
            clear_btn = gr.Button("ğŸ”„ æ¸…ç©ºå¯¹è¯")

        # ğŸ’» ç³»ç»Ÿç›‘æ§
        with gr.Accordion("ğŸ’» ç³»ç»Ÿç›‘æ§", open=False):
            gr.Markdown("### å®æ—¶ç³»ç»ŸæŒ‡æ ‡")
            diagnose_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€")
            status_panel = gr.JSON(label="ç³»ç»ŸçŠ¶æ€", value={"çŠ¶æ€": "æ­£åœ¨è·å–..."})

        interface.load(system_diagnosis, inputs=None, outputs=status_panel)

        # ğŸ¤– æ¨¡å‹ç®¡ç†
        with gr.Accordion("ğŸ¤– æ¨¡å‹ç®¡ç†", open=False):
            model_selector = gr.Dropdown(
                label="é€‰æ‹©æ¨¡å‹",
                choices=["deepseek-r1-distill-qwen-1.5b", "another-model-name"],  # åˆ—å‡ºAPIæ”¯æŒçš„æ¨¡å‹
                value=config.LLM_MODEL_NAME  # é»˜è®¤å€¼ä¸ºå½“å‰æ¨¡å‹
            )
            model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False, value=f"å½“å‰æ¨¡å‹: {config.LLM_MODEL_NAME}")

        interface.load(lambda: switch_model(config.LLM_MODEL_NAME), inputs=None, outputs=model_status)

        # ğŸ’¬ å¯¹è¯å†å²
        with gr.Accordion("ğŸ’¬ å¯¹è¯å†å²", open=False):
            export_btn = gr.Button("å¯¼å‡ºå†å²")
            import_btn = gr.UploadButton("å¯¼å…¥å†å²", file_types=[".json"])
            export_btn.click(export_chat_history, inputs=chatbot, outputs=gr.File())
            import_btn.upload(import_chat_history, inputs=import_btn, outputs=chatbot)

        # ğŸ“Š ç”Ÿæˆå‚æ•°
        with gr.Accordion("ğŸ“Š ç”Ÿæˆå‚æ•°", open=False):
            max_tokens = gr.Slider(128, 4096, value=512, label="ç”Ÿæˆé•¿åº¦é™åˆ¶")
            temperature = gr.Slider(0.1, 1.0, value=0.7, label="åˆ›é€ æ€§")
            top_p = gr.Slider(0.1, 1.0, value=0.9, label="æ ¸å¿ƒé‡‡æ ·")
            topk_retrieval = gr.Slider(1, 10, value=3, step=1, label="æ£€ç´¢æ–‡æ¡£æ•°é‡ top_k")
            dist_threshold = gr.Slider(0.0, 1.0, value=0.3, step=0.05, label="æ£€ç´¢è·ç¦»é˜ˆå€¼")
            show_debug = gr.Checkbox(label="æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹", value=True)

        # ç»‘å®šäº‹ä»¶
        msg_input.submit(
            chat_with_rag,
            inputs=[msg_input, chatbot, max_tokens, temperature, top_p, show_debug, topk_retrieval, dist_threshold],
            outputs=[msg_input, chatbot]
        )
        submit_btn.click(
            chat_with_rag,
            inputs=[msg_input, chatbot, max_tokens, temperature, top_p, show_debug, topk_retrieval, dist_threshold],
            outputs=[msg_input, chatbot]
        )

        clear_btn.click(lambda: [(None, "å¯¹è¯å·²æ¸…ç©º")], outputs=chatbot)

        upload_btn.upload(upload_files, inputs=[upload_btn, chatbot], outputs=[chatbot])

        diagnose_btn.click(system_diagnosis, outputs=status_panel)
        model_selector.change(switch_model, inputs=model_selector, outputs=model_status)

    return interface


# å¯åŠ¨ Gradio
if __name__ == "__main__":
    interface = create_gradio_interface()
    interface.launch(server_name="127.0.0.1", server_port=7860, share=True)
